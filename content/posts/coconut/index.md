---
title: "ğŸ¥¥ CoCoNut; Training Large Language Models to Reason in a Continuous Latent Space (Meta, 2024)"
description: "Chain of Continuous Thought"
summary: ""
date: 2025-01-24T00:00:00+09:00
lastmod: 2025-12-09T10:21:13+09:00
draft: false
weight: 50
categories: ["LLM", "CoT", "paper review"]
tags: ["CoCoNut", "Meta", "LLM"]
contributors: []
pinned: false
homepage: false
seo:
  title: "" # custom title (optional)
  description: "" # custom description (recommended)
  canonical: "" # custom canonical URL (optional)
  noindex: false # false (default) or true
---

## 1/ Chain-Of-Thought (CoT)

**CoT í•œê³„**: LLMì˜ reasoningì´ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ìƒì„±ë˜ì–´ì•¼ í•œë‹¤ëŠ” ì ì€ ì œì•½ì„ ê°€í•  ìˆ˜ ìˆë‹¤.

Neuroimaging ì—°êµ¬ì— ì˜í•˜ë©´ ì–¸ì–´ ì´í•´ ë° ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” ì¸ê°„ ë‘ë‡Œ ì˜ì—­ì´ ì¶”ë¡  ê³¼ì • ì¤‘ì—ëŠ” ë¹„í™œì„±í™”ëœë‹¤ê³  í•¨. ì´ëŠ” ì–¸ì–´ëŠ” communicationì— ì í•©í•  ë¿ ë³µì¡í•œ ë¬¸ì œ í•´ê²°ì—ëŠ” ë¶ˆí•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬í•œë‹¤.

ì¸ê°„ì´ ì¶”ë¡  ì¤‘ì— ìƒê°ì„ ì–¸ì–´ë¡œ ë°”ê¿”ì•¼ í•  í•„ìš”ê°€ ì—†ëŠ” ê²ƒì²˜ëŸ¼ AIë„ ë§ˆì°¬ê°€ì§€ì´ë‹¤. (ì¸ê³µì§€ëŠ¥ì´ ì¸ê°„ì„ ë”°ë¼í•œë‹¤ëŠ” ê²ƒì´ ë§ì´ ëŠê»´ì§€ëŠ” ë¶€ë¶„) â†’ LLMë„ language space ëŒ€ì‹  latent spaceì—ì„œ reasoningì„ ìˆ˜í–‰í•  í•„ìš”ê°€ ìˆë‹¤.
ëª¨ë¸ì´ ìƒì„±í•˜ëŠ” ëŒ€ë¶€ë¶„ì˜ tokensëŠ” í…ìŠ¤íŠ¸ì˜ ì¼ê´€ì„±ì„ ìœ ì§€í•˜ëŠ” ë° í•„ìš”í•  ë¿ ì‹¤ì œë¡œ ì¶”ë¡ ì— í¬ê²Œ ê¸°ì—¬í•˜ì§€ ì•ŠëŠ”ë‹¤.

ì–¸ì–´ ì œì•½ ì—†ì´ ììœ ë¡œìš´ ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ë„ë¡. í•„ìš”í•  ë•Œë§Œ ê²°ê³¼ë¥¼ ì–¸ì–´ë¡œ ë²ˆì—­í•  í•„ìš”ê°€ ìˆë‹¤.

ë³¸ ì—°êµ¬ëŠ” word-based reasoningì˜ ì œì•½ì—ì„œ ë²—ì–´ë‚˜ LLMì´ continuous latent spaceì—ì„œ ì¶”ë¡ í•´ì•¼í•œë‹¤ê³  ì œì•ˆí•œë‹¤. í•´ë‹¹ methodë¥¼ CoCoNUT (Chain of Continuous Thought)ë¼ê³  ë¶€ë¥¸ë‹¤.

## 2/ CoT vs CoCoNUT (Chain of Continuous Thought)

<img src="image_1.png" alt="image_1.png" style="width:600px;height:auto;" />

CoTëŠ” ì¶”ë¡  ê³¼ì •ì„ word token sequenceë¡œ ìƒì„±í•˜ëŠ” ë°˜ë©´, CoCoNUTëŠ” last hidden stateë¥¼ reasoning state(continuous thought)ë¡œ í‘œí˜„í•˜ì—¬ next input embeddingìœ¼ë¡œ ì§ì ‘ ì´ìš©í•œë‹¤.

â†’ LLMì´ language spaceê°€ ì•„ë‹ˆë¼ ì œì•½ì´ ì—†ëŠ” latent spaceì—ì„œ ì¶”ë¡ í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

## 3/ CoT Method

question â†’ LLMì— input tokensë¡œ embeddingë˜ì–´ input.

â†’ responseì˜ ì²« ë²ˆì§¸ tokenì„ ë°›ìŒ (ì¶”ë¡  ê³¼ì • ì‹œì‘), í•´ë‹¹ tokenì€ last hidden stateì—ì„œ ê°€ì ¸ì˜´. (ì¦‰, backbone Transformerì˜ ë§ˆì§€ë§‰ ë ˆì´ì–´ì˜ ì¶œë ¥)

â†’ forward pass ë°˜ë³µ, í˜„ì¬ stageê¹Œì§€ ê°€ì§„ reasoning process tokensê³¼ questionì„ ê³µê¸‰

## 4/ Coconut Method

language modeì—ì„œ latent thought modeë¡œ ë³€ê²½.

ëª¨ë¸ì€ ê¸°ë³¸ language modelë¡œ ì‘ë™í•˜ë©´ì„œ next tokenì„ ìƒì„±.

latent modeì—ì„œ last hidden stateë¥¼ next stepì˜ inputìœ¼ë¡œ ì‚¬ìš©.

last hidden stateëŠ” current reasoning stateë¥¼ ë‚˜íƒ€ë‚´ê³ , ì´ë¥¼ â€˜continuous thoughtâ€™ë¼ê³  í•¨.

`<bot>` special token + question \*`<bot>` : latent thought mode ì‹œì‘

â†’ questionì„ ì²˜ë¦¬í•˜ê³  last hidden stateë¥¼ ìƒì‚° (ì´ì „ì—ëŠ” language tokenìœ¼ë¡œ ë°”ê¿¨ëŠ”ë°, ì—¬ê¸°ì„œëŠ” ì•„ë‹˜)

ëŒ€ì‹  hidden stateê°€ ë‹¤ì‹œ ëª¨ë¸ì— input embeddingìœ¼ë¡œ questionì˜ embeddingsì™€ special tokenê³¼ í•¨ê»˜ ë“¤ì–´ê°„ë‹¤.

â†’ ë°˜ë³µ

â†’ ë°˜ë³µí•˜ë©´ì„œ ì ì  ë” ë§ì€ thought tokensë¥¼ inputìœ¼ë¡œ í™œìš©

â†’ `<eot>` special token ì‚¬ìš© \*`<eot>` : latent thought mode ì¢…ë£Œ ë° language mode ì‹œì‘

## 5/ Training

<img src="image_2.png" alt="image_2.png" style="width:600px;height:auto;" />

continuous latent spaceì—ì„œ ì¶”ë¡ í•˜ëŠ” ë²•ì„ LLMì—ê²Œ ì–´ë–»ê²Œ í›ˆë ¨í• ê¹Œ?

Stage 0ì—ì„œ ëª¨ë¸ì€ thought tokensë¥¼ ìƒì„±í•˜ì§€ ì•ŠëŠ”ë‹¤ë‹¤. CoT samplesìœ¼ ë‹µì„ ë§ì¶”ë„ë¡, reasoning tracesë¥¼ ìƒì‚°í•˜ë„ë¡ í›ˆë ¨ë˜ì–´ ìˆì„ ë¿ì´ë‹¤.

ê·¸ ì´í›„ ê° stageì—ì„œ sampleë¡œë¶€í„° í•˜ë‚˜ì˜ reasoning stepì„ ì œê±°í•˜ê³ , ëŒ€ì‹ ì— thought tokensì„ ë„£ëŠ”ë‹¤. (í•œí•œ ê°œì˜ singe reasoning step ëŒ€ì‹  í•œ ê°œì˜ thought tokensì´ ê° stageì— ì¶”ê°€, hyperparametr `c`ë¡œ ì»¨íŠ¸ë¡¤)

\*c : í•œ ë²ˆì˜ ì¶”ë¡  ë‹¨ê³„ì—ì„œ ìƒì„±í•  ìˆ˜ ìˆëŠ” thought (continuous embeddings)ì˜ ê°œìˆ˜

ê° stageë§ˆë‹¤ ë‚¨ì•„ ìˆëŠ” reasoning stepsê³¼ answerì— ê´€í•´ì„œë§Œ loss ê³„ì‚°í•œë‹¤. (thought tokenëŠ” loss ê³„ì‚° ì•ˆí•¨) ê° passë§ˆë‹¤ ìƒˆë¡œìš´ latent thoughtë¥¼ ê³„ì‚°í•˜ê³ , ë‚¨ì•„ ìˆëŠ” text sequenceì— ëŒ€í•´ì„œ lossë¥¼ ì–»ëŠ”ë‹¤.

loss objectiveëŠ” continuous thoughtê°€ language thoughtë¥¼ ì••ì¶•í•˜ê¸°ë³´ë‹¤ reasoningì˜ ì˜ˆì¸¡ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ì§‘ì¤‘í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ëª¨ë¸ì´ human languageì™€ ë¹„êµí–ˆì„ ë•Œ ë” íš¨ìœ¨ì ì¸ í‘œí˜„ì„ ë°°ìš¸ ìˆ˜ ìˆë‹¤.
language tokensë¥¼ ìƒì„±í•  í•„ìš” ì—†ì´ ë‚´ë¶€ì ìœ¼ë¡œ ê³„ì† ì¶”ë¡ í•  ìˆ˜ ìˆë„ë¡ í•œë‹¤.

## 6/ Switching

ëª¨ë¸ì€ ì–¸ì œ latent thought modeì—ì„œ language modeë¡œ ë°”ë€ŒëŠ”ì§€ ì–´ë–»ê²Œ ì•Œê¹Œ?

- ëª¨ë¸ì´ latent thoughtsì— ê¸°ë°˜í•´ binary classifier ì‚¬ìš©ì„ ê²°ì •í•˜ëŠ” ê²ƒì„ ë‚´ë²„ë ¤ë‘ê¸°
- latent thoughtsì˜ ì¼ì •í•œ ìˆ˜ë¥¼ ì‚¬ìš©

ë‘ ì „ëµ ëª¨ë‘ ìœ ì‚¬í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤¬ì§€ë§Œ, ë‹¨ìˆœì„±ì„ ì´ìœ ë¡œ constant number of thoughtsë¥¼ íƒí•œë‹¤.

## 7/ Results

<img src="image_3.png" alt="image_3.png" style="width:600px;height:auto;" />

GPT-2ë¥¼ ê¸°ë³¸ ëª¨ë¸ë¡œ ì‚¬ìš©.

- No-CoT ëŒ€ë¹„
  ëª¨ë“  ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ ìš°ì„¸
- CoT ëŒ€ë¹„
  ìˆ˜í•™ (GSM8k; ì´ˆë“±í•™ ìˆ˜ì¤€ ìˆ˜í•™ ë¬¸ì œ) ì€ CoTê°€ ë” ìš°ì„¸. ê·¸ëŸ¬ë‚˜ CoTëŠ” Coconutì— ë¹„í•´ ë” ë§ì€ tokensë¥¼ í•„ìš”ë¡œ í•¨. â†’ Coconutì´ í›¨ì”¬ íš¨ìœ¨ì .
  ProsQAì™€ ê°™ì´ ë‹¨ê³„ì  ì‚¬ê³ ê°€ ë” í•„ìš”í•œ ë°ì´í„°ì…‹ì—ì„œ ìš°ì„¸.
- i-CoT (ì¶”ë¡  ê°€ì •ì„ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ë‚´ë¶€í™”í•˜ë ¤ëŠ” ë°©ë²•) ëŒ€ë¹„
  ìœ ì‚¬í•˜ë‚˜ ìˆ˜í•™ì€ coconutì´ ë” ìš°ì„¸. ê·¸ëŸ¬ë‚˜ i-CoTëŠ” ë” ì ì€ tokens ì‚¬ìš©

w/o curriculumëŠ” multi-stage traniningì˜ ì¤‘ìš”ì„±ì„ ë³´ì—¬ì¤€ë‹¤.

### 8/ BFS-like Reasoning

<img src="image_4.png" alt="image_4.png" style="width:600px;height:auto;" />

<img src="image_5.png" alt="image_5.png" style="width:600px;height:auto;" />

ProsQA ë°ì´í„°ì…‹ê³¼ ê°™ì€ planning-intensive taskì—ì„œ Coconutì´ CoTë³´ë‹¤ ìš°ì„¸í•œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤.

CoTëŠ” ì¶”ë¡  ì¤‘ hallucinated edgeë¥¼ ìƒì„±í•´ ì˜ëª»ëœ ê²°ê³¼ë¥¼ ë„ì¶œí–ˆê³ , Coconutì€ thought tokensë¥¼ ì‚¬ìš©í•´ BFSì™€ ë¹„ìŠ·í•˜ê²Œ ì—¬ëŸ¬ ê²½ë¡œ íƒìƒ‰í•œë‹¤. thought tokenë¥¼ í•˜ë‚˜ ì‚¬ìš©í•  ë•ŒëŠ” ì˜¤ë‹µì´ì§€ë§Œ, ë‘ ê°œë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ë” ë§ì€ ê²½ë¡œë¥¼ ì°¾ê³  ì˜¬ë°”ë¥¸ ë‹µ ë„ì¶œí•œë‹¤.
ë‹¨ì¼ ê²½ë¡œê°€ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ê²½ë¡œë¥¼ í†µí•´ íƒìƒ‰í•œë‹¤ëŠ” ì ì— BFSì™€ ë¹„ìŠ·í•˜ë‹¤.

## 9/ Takeaways

<img src="image_6.png" alt="image_6.png" style="width:300px;height:auto;" />

Auto modeì—ì„œëŠ” LLMì€ planningì„ í•˜ì§€ ëª»í•œë‹¤.

CoT, ReACT (Reasoning + Acting)

ì‚¬ëŒì´ ê³„ì† í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•˜ëŠ” ê³¼ì •ì—ì„œ ëª¨ë¸ì´ ì •ë‹µì„ ë§ì¶”ëŠ” ê²ƒê³¼ ê°™ì€ Clever Hans íš¨ê³¼ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

\*Clever Hans : LLM ìì²´ ê²€ì¦ ë° ê°œì„  ëŠ¥ë ¥ì´ ë¶€ì¡±í•˜ì—¬ ë…¼ë¦¬ì ìœ¼ë¡œ ë¬¸ì œ í•´ê²°í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í”„ë¡¬í”„íŠ¸, ì¦‰ shallow heuristicì— ì˜ì¡´í•˜ë©´ì„œ ë‹µì„ ë‚´ë†“ëŠ” ê²ƒ.

[Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models](https://arxiv.org/abs/2305.14763)

(ë‹¤ë§Œ í•´ë‹¹ ë…¼ë¬¸ì—ì„œ 4o, o1ì€ ì—†ëŠ” ì  ì°¸ê³ í•´ì•¼ í•  ê²ƒ ê°™ë‹¤... N-ToM;Neural Theory of Mindê³¼ ê°™ì´ ì¸ê°„ ìˆ˜ì¤€ì˜ ì •ì„œì  ì¶”ë¡  ëŠ¥ë ¥ì„ ëª¨ë°©í•˜ê¸° ìœ„í•´ì„œëŠ” ì•„ì§ ê°œì„ ì´ í•„ìš”..?)

## Reference

[Training Large Language Models to Reason in a Continuous Latent Space](https://arxiv.org/abs/2412.06769)

[facebookresearch/coconut](https://github.com/facebookresearch/coconut)

[Chain of Continuous Thought (AIPapers Academy)](https://aipapersacademy.com/chain-of-continuous-thought/)

[ë©”íƒ€ì˜ ì½”ì½”ë„›(COCONUT): ì–¸ì–´ ì—†ì´ ìƒê°í•˜ëŠ” AI ë°©ë²• (AIë„·)](https://www.ainet.link/17963)
