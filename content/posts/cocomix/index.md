---
title: "ğŸï¸ CoCoMix; LLM Pretraining with Continuous Concepts (Meta, 2025)"
description: ""
summary: ""
date: 2025-03-07T00:00:00+09:00
lastmod: 2025-12-08T13:49:46+09:00
draft: false
weight: 50
categories: ["paper review"]
tags: ["CoCoMix", "META", "LLM", "Continuous Concept", "SAE" ]
contributors: []
pinned: false
homepage: false
seo:
  title: "" # custom title (optional)
  description: "" # custom description (recommended)
  canonical: "" # custom canonical URL (optional)
  noindex: false # false (default) or true
---

<img src="image_1.png" alt="image_1.png" style="width:150px;height:auto;" />

## **CoCoMix (Continuous Concept Mixing)**

next token predictionê³¼ continuous conceptsë¥¼ ê²°í•©í•œ í”„ë ˆì„ì›Œí¬.

- pretrained sparse autoencoderë¥¼ í†µí•œ concept ì¶”ì¶œ.
- continuous conceptë¥¼ hidden stateì— í˜¼í•©
  â†’ discrete language tokens ëŒ€ì‹  continuous latent representationsìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ì ‘ê·¼ ë°©ì‹.
  (LLMì´ ë³¸ì§ˆì ìœ¼ë¡œ high-level conceptê³¼ reasoning ê°€ì •ì„ latent representationsì— ë‚´ì¬í•˜ê³  ìˆë‹¤ëŠ” ê²ƒì„ ì„¤ëª…)

## 1/ Problem

- ì¼ë°˜ì ìœ¼ë¡œ LLMì€ token-levelì—ì„œ í•™ìŠµë¨. ì£¼ì–´ì§„ contextì— ë”°ë¼ ê°€ì¥ ì ì ˆí•œ next tokenì„ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµ â†’ `the`, `a`, `and` ë“±ê³¼ ê°™ì€ ê¸°ëŠ¥ì–´ (function words â†” content words)ì²˜ëŸ¼ í”¼ìƒì ì¸ ë‹¨ì–´ë“¤ì´ ìˆì–´ ëª¨ë¸ì´ reasoningí•˜ê¸° ìœ„í•´ì„œ (ì‹¬ì¸µì ì¸ ì˜ë¯¸ ì´í•´)ëŠ” ë§ì€ í›ˆë ¨ì´ í•„ìš”í•¨.

## 2/ Solution

SAE (Sparse Autoencoder)ë¥¼ ì‚¬ìš©í•´ ì˜ë¯¸ ìˆëŠ” conceptë¥¼ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ ëª¨ë¸ì˜ hidden stateì— ê²°í•©. conceptì€ next token predictionì— ì§ì ‘ì ìœ¼ë¡œ ê¸°ì—¬í•˜ê²Œ ë¨. (ê° contextì— ëŒ€í•´ ì˜ë¯¸ ìˆëŠ” conceptë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì¶œí•´ í‘œí˜„)

**ê¸°ì¡´ ë°©ì‹** - Next Token Prediction
<img src="image_2.png" alt="image_1.png" style="width:600px;height:auto;" />

**_CoCoMix_** - Next Token Prediction + Continuous Concept Prediction
<img src="image_3.png" alt="image_1.png" style="width:600px;height:auto;" />

- **Target concept selection using attribution**
  ì¶”ì¶œëœ conceptë“¤ì€ ê°œë³„ì ìœ¼ë¡œ inputì— ì–´ëŠ ì •ë„ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ í‰ê°€í•˜ê¸° ìœ„í•´ attribution score (i.e. measuring the influence on the output)ë¥¼ ì‚¬ìš©í•´ ê°€ì¥ ì¤‘ìš”í•œ concept ì„ íƒ.

- **Predicting the selected concepts**
  ì„ íƒëœ conceptì€ ëª¨ë¸ì˜ hidden stateë¡œë¶€í„° ì˜ˆì¸¡ë˜ë©°, ì´ë•Œ Cross-Entropy Lossë¥¼ ìµœì†Œí™”í•˜ë©° í•™ìŠµ. (ê¸°ì¡´ next token prediction lossì™€ concept prediction lossë¥¼ ì¡°í•©í•´ ìµœì í™”)

- **Mixing continuous concepts with token embeddings**
  ì˜ˆì¸¡ëœ ì—°ì† conceptë“¤ì„ ì••ì¶•í•´ í•˜ë‚˜ì˜ â€œì—°ì† conceptâ€ (compact vector)ì„ í˜•ì„±í•˜ê³ , hidden stateì— interleaving (ë¼ì›Œ ë„£ê¸°) â†’ tokenê³¼ ì—°ì† conceptì´ í•¨ê»˜ ëª¨ë¸ì˜ ì˜ˆì¸¡ì— ê¸°ì—¬í•˜ê²Œ ë¨.

- CoCoMixëŠ” ì˜ˆì¸¡ëœ conceptì„ ì§ì ‘ ë¶„ì„í•˜ê³  ìˆ˜ì •í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•´ ìœ ì €ê°€ ëª¨ë¸ì˜ reasoning ê³¼ì •ì„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆìŒ (ì§ì ‘ì ìœ¼ë¡œ ìƒì„± ê²°ê³¼ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥)

## 3/ Concept Prediction Loss

$$
\mathcal L_{concept}(a_t) = \frac{1}{K_{attr}} \sum_{i \in \mathcal I} CE(z_t, i)
$$

$a_t$ - attribution score

$h_t = h(\mathbf x)_t \in \mathbb R^d$ - input $\mathbf x$ì— ëŒ€í•œ ëª¨ë¸ì´ ìƒì„±í•œ hidden state

$M( \cdot )$ - linear prediction head

$z_t = M(h_t) = Wh_t + b \in \mathbb R^C$ - model outputs (logit)

context ì •ë³´ë¥¼ ê°–ê³  ìˆëŠ” hidden state representation (ê³ ì°¨ì› ë²¡í„°)ë¥¼ ì„ í˜• ë³€í™˜í•´ $C$ì°¨ì›ì˜ concept spaceì— íˆ¬ì˜í•´ ê° conceptë³„ logit ê°’ ê³„ì‚°.

$\mathcal I = \{i_1, ... , i_{K_{attr}}\}$ - ìƒìœ„ $K_{attr}$ ê°œë…ë“¤ì˜ index ì§‘í•©.

## 4/ Continuous Concept Vector

concept predictioní•  ë•Œ ë‹¨ì¼ conceptì´ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ê°œì˜ conceptì„ í•œ ë²ˆì— ì˜ˆì¸¡í•˜ê³ , ì´ë¥¼ í•˜ë‚˜ì˜ ì—°ì†ì ì¸ ë²¡í„°ë¡œ ì••ì¶•.

ê·¸ë˜ì„œ ë‹¨ìˆœ í™•ë¥  ê°’ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í•™ìŠµ ê°€ëŠ¥í•œ ë²¡í„° í˜•íƒœë¡œ ë³€í™˜í•´ hidden stateì™€ ê²°í•©.

$$
\hat c_t = W \cdot \text{TopK}(z_t) + b
$$

$\hat c_t \in \mathbb R^d$ , $W \in \mathbb R^{d \times C}$, $b \in \mathbb R^d$ (TopK-sparse vectorë¥¼ $d$-dimensional embeddingì— íˆ¬ì˜)

- **Top K Activation**
  concept prediction logit $z_t$ë¥¼ í¬ì†Œí™” (sparsify)í•´ ê°€ì¥ ì¤‘ìš”í•œ conceptë§Œ ì„ ë³„.
- continuous concept vector $\hat c_t$ë¡œ ì••ì¶•
- $\hat c_t$ë¥¼ $h_t$ì™€ ê²°í•©í•´ í•¨ê»˜ ì „ë‹¬.

### Training objective

ê¸°ì¡´ Next Token Prediction loss + Concept Prediction loss

$$
\sum^{T-1}_{t=1} - log f(x_{t +1} | h_{\leq t}, \hat c {\leq t}) + \lambda \mathcal L_{concept}(a_t)
$$

$f(x_{t +1} | h_{\leq t}, \hat c {\leq t})$ - ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥  ë¶„í¬ì—ì„œ ë‹¤ìŒ í† í° $x_{t+1}$ì´ ë“±ì¥í•  í™•ë¥ 
(CE Lossë¥¼ í†µí•´ ì˜ˆì¸¡ ë¶„í¬ì™€ ì‹¤ì œ ë¶„í¬ ì°¨ì´ë¥¼ ìµœì†Œí™”)

$\lambda$ - a tunable hyperparameter (concept prediction lossì˜ ë¹„ì¤‘ ì¡°ì ˆ)

## 5/ Architecture

<img src="image_4.png" alt="{439ECC5F-1D79-4A95-9871-1E7F6EA0B7AC}.png" style="width:600px;height:auto;" />

## 6/ Results

- sample efficient (ì ì€ ë°ì´í„°ë¡œë„ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ ë‹¬ì„± ê°€ëŠ¥)
- ê¸°ì¡´ NTP (Next Token Prediction)ë³´ë‹¤ ëŠ¥ê°€
- weak-to-strong supervision ì‹œë‚˜ë¦¬ì˜¤ì—ì„œë„ ì„±ëŠ¥ ê°œì„ .
  weak supervisionì€ ë¶ˆì™„ì „í•œ labelì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°˜ë©´, strong supervisionì€ ëª…í™•í•˜ê³  ì •í™•í•œ labelì„ ì‚¬ìš©í•´ ëª¨ë¸ í›ˆë ¨.
- CoCoMixëŠ” ì‘ì€ ëª¨ë¸ì—ì„œ ì¶”ì¶œí•œ conceptì„ í™œìš©í•´ í° ëª¨ë¸ì„ í›ˆë ¨í•´, ë¶ˆì™„ì „í•œ ì •ë³´ë¥¼ í™œìš©í•´ë„ íš¨ê³¼ì ì¸ ì„±ëŠ¥ í–¥ìƒ

<img src="image_5.png" alt="{80F40B04-D984-4EDA-AD9A-6375DDBB9E6F}.png" style="width:600px;height:auto;" />

ê¸°ì¡´ NTP ëª¨ë¸ê³¼ ë¹„êµí–ˆì„ ë•Œ Perplexityê°€ ë” ë‚®ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ

CoCoMixëŠ” 21.5% ë” ì ì€ tokensë¥¼ ì‚¬ìš©í•¨ì—ë„ ê¸°ì¡´ NTP ëª¨ë¸ê³¼ ë™ì¼í•œ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ

<img src="image_6.png" alt="{8577757F-FC3E-4402-B28D-A8D707617894}.png" style="width:600px;height:auto;" />

KD (Knowledge Distillation) - Teacher modelì˜ ì§€ì‹ì„ Student modelì— ì „ì´í•˜ëŠ” ë°©ë²•

_ìœ ì‚¬ì _ - pretrained model (SAE)ì—ì„œ ì˜ë¯¸ ìˆëŠ” ê°œë…ì„ ì¶”ì¶œí•´ base model í•™ìŠµì— í™œìš©í–ˆê¸° ë•Œë¬¸.

_ì°¨ì´ì _ - ê¸°ì¡´ KDëŠ” í™•ë¥  ê¸°ë°˜ í•™ìŠµì„ í–ˆë‹¤ë©´, CoCoMixëŠ” concept ê¸°ë°˜ í•™ìŠµì„ ìˆ˜í–‰í•´ weak-to-strong supervisionì„ ê°€ëŠ¥í•˜ê²Œ í•¨ (ì‘ì€ ëª¨ë¸ì—ì„œ ì¶”ì¶œí•œ ê°œë…ìœ¼ë¡œ ë” í° ëª¨ë¸ì„ í›ˆë ¨í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì…ì¦)

- ê¸°ì¡´ KD - â€œí•´ë‹¹ í† í°ì´ ë“±ì¥í•  í™•ë¥  ëª‡ %ì¸ì§€â€ ê°™ì€ í™•ë¥  ìì²´ë¥¼ ê·¸ëŒ€ë¡œ í•™ìŠµ
- CoCoMix - â€œí•´ë‹¹ ë¬¸ì¥ì—ì„œ ì¤‘ìš”í•œ ê°œë…ì´ ë¬´ì—‡ì¸ì§€â€ ê°™ì€ conceptë¥¼ í•™ìŠµí•œ í›„, hidden stateì— ê²°í•©

## 7/ Interpretability (í•´ì„ ê°€ëŠ¥ì„±) & Steerability (ì œì–´ ê°€ëŠ¥ì„±)

<img src="image_7.png" alt="{875DD0E5-2BD2-4C0B-B5C6-6D691D462D45}.png" style="width:600px;height:auto;" />

CoCoMix vs. GPT2-SAE (GPT2ì— SAE concept space ì¶”ê°€)

ê²°ë¡ ì ìœ¼ë¡œ í•´ì„ ê°€ëŠ¥ì„± ì¦ê°€í•¨.

- ëª¨ë¸ì´ ì–´ë–¤ ê°œë…ì— ë” ì§‘ì¤‘í•˜ëŠ”ì§€ ë¶„ì„ ê°€ëŠ¥.
- ì˜ˆì¸¡ ê°œë…ì˜ activationì„ ì¡°ì ˆí•´ (íŠ¹ì • ê°œë… ì¡°ì •)
  íŠ¹ì • ê°œë…ì„ ë°˜ì˜í•˜ë„ë¡ ì¡°ì ˆí•  ìˆ˜ ìˆìŒ.
  â€™siteâ€™ë¥¼ ê°•ì¡°í•˜ë©´ â€˜siteâ€™ì™€ ê´€ë ¨ë˜ë„ë¡ ìƒì„±í•˜ëŠ” ë“± ë” ì˜ë¯¸ ìˆëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ.

## 8/ Future Work

í˜„ì¬ëŠ” pretrained SAEë¥¼ ì‚¬ìš©í•´ ì˜ë¯¸ ìˆëŠ” ê°œë…ì„ ì¶”ì¶œí•˜ê³  ë‹¤ì‹œ í•™ìŠµí•˜ëŠ” ë°©ì‹ì´ì§€ë§Œ,
í–¥í›„ì—ëŠ” ëª¨ë¸ ìì²´ì ìœ¼ë¡œ ì—°ì† ê°œë…ì„ í•™ìŠµí•˜ëŠ” ë°©ë²• ì—°êµ¬í•  ê³„íšì„ ë³´ì„.
Knowledge Distillation ì—†ì´ë„ ê°œë…ì„ ì§ì ‘ í•™ìŠµí•˜ê³ ì í•¨.

## Reference

- [CoCoMix: LLM Pretraining with Continuous Concepts (Arxiv)](https://arxiv.org/pdf/2502.08524)

- [GitHub - facebookresearch/RAM/projects/cocomix](https://github.com/facebookresearch/RAM/tree/main/projects/cocomix)

- [AI Papers Academy - CoCoMix](https://aipapersacademy.com/cocomix/)
