---
title: "ğŸŒ³ LLaDA; Large Language Diffusion Models (2025-02)"
description: ""
summary: ""
date: 2025-03-21T00:00:00+09:00
lastmod: 2025-12-09T10:22:13+09:00
draft: false
weight: 50
categories: ["paper review"]
tags: ["diffusion language model", "LLM", "LDM" ]
contributors: []
pinned: false
homepage: false
seo:
  title: "" # custom title (optional)
  description: "" # custom description (recommended)
  canonical: "" # custom canonical URL (optional)
  noindex: false # false (default) or true
---

ìµœê·¼ ë“±ì¥í•œ Diffusion Modelsë“¤ì€ ARMs (Autoregressive Models) ë§Œí¼ ì„±ëŠ¥ì´ ê´œì°®ì€ í¸ì´ê³ , context-awareness ì˜ì—­ì—ì„œëŠ” ì„±ëŠ¥ì´ ë”  ê°•í•˜ë‹¤ëŠ” í‰ì´ ë‚˜ì˜¤ê³  ìˆë‹¤.
â†’ DLMsì´ ì „í†µ ARMs ëŒ€ì²´í•  ìƒˆë¡œìš´ ëŒ€ì•ˆìœ¼ë¡œ ë¶€ìƒí•˜ê³  ìˆëŠ” ê²ƒ ê°™ë‹¤.

<img src="image_1.png" alt="image_1.png" style="width:300px;height:auto;" />

## 1/ Background : Diffusion Models

<img src="image_2.png" alt="image_2.png" style="width:500px;height:auto;" />

Diffusion Modelì€ generative modelsì˜ í•œ ì¢…ë¥˜ë¡œ,
random noiseë¥¼ ì ì§„ì ìœ¼ë¡œ ì¶”ê°€ (forward), ì ì§„ì ìœ¼ë¡œ denoise (reverse)í•˜ë©´ì„œ ë°ì´í„°ë¥¼ ìƒì„±í•œë‹¤.

ë‹¤ì–‘í•œ noiseë¥¼ ì¶”ê°€í•˜ê³ , ì´ë¥¼ ë³µì›í•˜ëŠ” ë²•ì„ ëª¨ë¸ì´ í•™ìŠµí•˜ë„ë¡ í›ˆë ¨ì‹œí‚¤ëŠ” ë°©ë²•ì´ë‹¤. ìƒì„± ì‹œì—ëŠ” random inputì„ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•´ ê²°ê³¼ë¥¼ ì ì°¨ ì •ì œí•˜ë©°, íš¨ìœ¨ì ì¸ samplingì„ í†µí•´ ì ì€ stepìœ¼ë¡œë„ ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤. ì´í›„ conditioning, guidance ê¸°ë²•ì„ í†µí•´ ì¶œë ¥ ì¡°ì ˆì´ ê°€ëŠ¥í•˜ë‹¤.

ì´ë¯¸ì§€ ìƒì„± ë¶„ì•¼ì—ì„œ ì„±ê³µì ì´ì—ˆëŠ”ë°, ìµœê·¼ í…ìŠ¤íŠ¸ ìƒì„±ì—ë„ ì ìš©ë˜ëŠ” ì›€ì§ì„ì„ ë³´ì´ê³  ìˆë‹¤.

ARMsì€ ìˆœì°¨ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ë°˜ë©´, diffusion ëª¨ë¸ì€ í…ìŠ¤íŠ¸ ì „ì²´ë¥¼ ë™ì‹œì— ìƒì„±í•˜ê³ , ë°˜ë³µì ì¸ ìˆ˜ì • ê³¼ì •ì„ ê±°ì³ ìµœì¢… ì¶œë ¥ì„ ì™„ì„±í•œë‹¤. â†’  ìƒì„± ì†ë„, íš¨ìœ¨ì„± í–¥ìƒ e.g. Inception Labsì˜ Mercury

```python
# Set the noise scheduler
noise_scheduler = DDPMScheduler(
    num_train_timesteps=1000, beta_schedule="squaredcos_cap_v2"
)

# Training loop
optimizer = torch.optim.AdamW(model.parameters(), lr=4e-4)

losses = []

for epoch in range(30):
    for step, batch in enumerate(train_dataloader):
        clean_images = batch["images"].to(device)
        *# Sample noise to add to the images*
        noise = torch.randn(clean_images.shape).to(clean_images.device)
        bs = clean_images.shape[0]

        *# Sample a random timestep for each image*
        timesteps = torch.randint(
            0, noise_scheduler.num_train_timesteps, (bs,), device=clean_images.device
        ).long()

        *# Add noise to the clean images according to the noise magnitude at each timestep*
        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)

        *# Get the model prediction*
        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]

        *# Calculate the loss*
        loss = F.mse_loss(noise_pred, noise)
        loss.backward(loss)
        losses.append(loss.item())

        *# Update the model parameters with the optimizer*
        optimizer.step()
        optimizer.zero_grad()

    if (epoch + 1) % 5 == 0:
        loss_last_epoch = sum(losses[-len(train_dataloader) :]) / len(train_dataloader)
        print(f"Epoch:{epoch+1}, loss: {loss_last_epoch}")
```

## 2/ TL;DR

LLaDAëŠ” pre-trainingê³¼ supervised fine-tuning (SFT) ë°©ì‹ìœ¼ë¡œ scratchë¶€í„° í•™ìŠµëœ diffusion-based LLMìœ¼ë¡œ, masking (forward process)ì™€ denoising (reverse process - predicting masked tokens) ê³¼ì •ì„ í†µí•´ í™•ë¥ ì  ìƒì„± ëŠ¥ë ¥ì„ ê°–ì·„ë‹¤. ARMs (e.g.  LLaMA3-8B) ëŒ€ë¹„ ë†’ì€ scalability,ì™€ in-context learning ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, instruction-following ëŠ¥ë ¥ë„ ìš°ìˆ˜í•˜ë‹¤. íŠ¹íˆ reversal curse ë¬¸ì œë¥¼ ê·¹ë³µí•˜ë©° â€˜Reversal Poem Completionâ€™ taskì—ì„œ GPT-4oë³´ë‹¤ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. â†’ ARMsì˜ ëŒ€ì•ˆ ê°€ëŠ¥ì„±ì´ ë†’ì€ ê²ƒ ê°™ë‹¤.

### Case

`Prompt:Â *Explain what artificial intelligence is.*`

<img src="https://ml-gsai.github.io/LLaDA-demo/static/images/diff_normal_150ms.gif" style="width:590px;height:auto;" />

https://ml-gsai.github.io/LLaDA-demo/


## 3/ LLaDA (LargeÂ LanguageÂ Diffusion with mAsking)

<img src="image_3.png" alt="image_3.png" style="width:450px;height:auto;" />


### Idea

Diffusionì€ ì¼ë°˜ì ìœ¼ë¡œ ì´ë¯¸ì§€ ìƒì„±ì— ì‚¬ìš©ë˜ëŠ” ë°©ì‹ì´ë‹¤. â†’ í…ìŠ¤íŠ¸ì˜ ê²½ìš°, ì´ë¯¸ì§€ì²˜ëŸ¼ continuous spaceê°€ ì•„ë‹ˆë¯€ë¡œ, discrete latent spaceì—ì„œ diffusionì„ ìˆ˜í–‰í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ìš©í•œë‹¤.

- ì´ë¯¸ì§€ : ì—°ì†ì ì¸ í”½ì…€ê°’ (ì—°ì† í™•ë¥  ë¶„í¬) ë³´í†µ UNET ê¸°ë°˜
Gaussian Noise ì ì§„ì ìœ¼ë¡œ ì¶”ê°€ â†’ ë…¸ì´ì¦ˆ ì œê±°í•˜ë©´ì„œ ì´ë¯¸ì§€ ë³µì›
- í…ìŠ¤íŠ¸ : ì´ì‚°ì ì¸ í† í° (ì´ì‚° í™•ë¥  ë¶„í¬) **â€œdiscrete diffusionâ€** Transfomer ê¸°ë°˜
LLaDA :  0 ~ 1 ì‚¬ì´ì˜ ì„ì˜ì˜ masking ë¹„ìœ¨ì„ ì‚¬ìš©í•´ í† í°ì„ ì ì§„ì ìœ¼ë¡œ masking â†’ maskingëœ token ì˜ˆì¸¡í•˜ë©´ì„œ ë³µì›í•œë‹¤.

    $$
    P(x_{ti} | x_{i}) = \begin{cases}1 - t & \text{if} \; x_{ti} = x_{i} \\ t & \text{if} \; x_{ti} = [MASK]\end{cases}
    $$


### Methods

<img src="image_4.png" alt="image_4.png" style="width:500px;height:auto;" />

**LLaDA**ëŠ” **masked diffusion model**ë¡œ, pre-training, SFT(Supervised Fine-Tuning), sampling ì„¸ ê³¼ì •ìœ¼ë¡œ ë™ì‘ ê³¼ì •ì„ ë‚˜ëˆ  ë³¼ ìˆ˜ ìˆë‹¤.

- **Pre-training**
    - input sequenceì˜ ëª¨ë“  tokenì„ ë¬´ì‘ìœ„ë¡œ ë…ë¦½ì ìœ¼ë¡œ masking.
    - masking ratio $t$ëŠ” [0,1]ì—ì„œ ëœë¤ìœ¼ë¡œ ìƒ˜í”Œë§ (ê° tokenì€ $t$ì˜ í™•ë¥ ë¡œ maskingë¨)

        $t=1$ì—ì„œ ëª¨ë“  tokenì´ maskingë˜ê³ , ì´ ì‹œì ì— ìƒì„±ëœ sequenceë¥¼ $x_1$

    - ëª¨ë¸ì€ mask predictorë¡œ ê° positionì˜ masked tokenì„ ë³µì›í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµ.
    - diffusionì˜ forward processì™€ ìœ ì‚¬í•˜ê²Œ ë°ì´í„°ë¥¼ ì ì°¨ noise (mask)ë¡œ ë§Œë“œëŠ” ê³¼ì •.
- **SFT (Supervised Fine-Tuning)**

<img src="image_5.png" alt="image_5.png" style="width:250px;height:auto;" />

    - PromptëŠ” ê³ ì •, Response ë¶€ë¶„ë§Œ ë§ˆìŠ¤í¬
    - ëª¨ë¸ì€ maskingëœ response ë¶€ë¶„ì„ ì˜ˆì¸¡í•˜ë„ë¡ fine-tuning.
    - Mask prediction lossë¡œ í•™ìŠµ.
- **Sampling**
    - $t=1$ (ëª¨ë“  tokenì´ maskëœ ìƒíƒœ)ì—ì„œ ì‹œì‘í•˜ì—¬ $t=0$ê¹Œì§€ ì§„í–‰.
    - mask predictorë¥¼ í†µí•´ mask ë³µì› (maskingëœ ëª¨ë“  tokenì„ ë™ì‹œì— ì˜ˆì¸¡)
    maksingëœ tokenì´ ë¬´ì—‡ì¸ì§€ ì˜ˆì¸¡.
    - ì˜ˆì¸¡ í›„ ì¼ë¶€ tokenì€ unmaskë˜ê³  ì¼ë¶€ëŠ” ë‹¤ì‹œ remask (flexible remasking)
    - ì´ë¥¼ ë°˜ë³µí•´ ì ì§„ì ìœ¼ë¡œ $t=0$ (ëª¨ë“  tokenì´ unmaskëœ ìƒíƒœ) ì™„ì „íˆ ë³µì›.

- **Forward** process : ì ì§„ì  masking, **Reverse** process :  iterative unmasking
- Samplingì—ì„œ remasking & refine ê³¼ì •ì„ ê±°ì¹˜ë©° ë†’ì€ í’ˆì§ˆê³¼ long-context ì¸ì§€ ëŠ¥ë ¥ì„ í™•ë³´.
- ARMsì˜ next-token-generation ë°©ì‹ë³´ë‹¤ global context ì´í•´ë„ê°€ ë” ë†’ìŒ.

### Architecture

```python
ë§ˆìŠ¤í¬ëœ ì´ˆê¸° ìƒíƒœ (xâ‚; t=1)
     â”‚
     â–¼
Mask Predictor (Transformer)
     â”‚
ì˜ˆì¸¡ëœ í† í° (ì¼ë¶€ëŠ” ë‹¤ì‹œ ë§ˆìŠ¤í¬ë¨)
     â”‚
     â–¼
Remasking (Remasking ì „ëµì— ë”°ë¼)
     â”‚
     â–¼
ë°˜ë³µì  ì •ì œ (t: 1 -&gt; 0)
     â”‚
     â–¼
ìµœì¢… í…ìŠ¤íŠ¸ (xâ‚€; t=0)
```

- Transformer ê¸°ë°˜ mask predictor &  multi-head attention block í¬í•¨.

### Training

Training Objective

$$
L(\theta) \triangleq -E_{t, x_0, x_t} \left[ \frac{1}{tL} \sum_{i=1}^L \mathbb{1}[x_{it} = M] \log p_\theta (x_{i0}|x_t) \right]
$$

- random  timestep $t$  ì„ íƒ.
- ì›ì‹œ í…ìŠ¤íŠ¸ì— noise (mask) ì¶”ê°€.

    $q(x_t|x_0)$

- ëª¨ë¸ì´ maskingëœ token ì˜ˆì¸¡.

    $x_t$ë¥¼ ì…ë ¥ ë°›ì•„ maskingëœ ìœ„ì¹˜ì˜ ì›ë˜ tokenì„ ì˜ˆì¸¡.

- **Loss :  Cross-Entropy Loss**

    ì˜ˆì¸¡ëœ tokenê³¼ ì‹¤ì œ token ê°„ì˜ Cross-Entropy Lossë¥¼ ê³„ì‚°í•˜ê²Œ í•´ ëª¨ë¸ ì—…ë°ì´íŠ¸.


### Sampling (Inference)

1. ì´ˆê¸° ìƒíƒœ ì„¤ì •

     ëª¨ë“  tokenì´ maskingëœ ì´ˆê¸° sequence $x_1$ ì„¤ì •.

2. denoising ë°˜ë³µ $(t: 1 \rightarrow 0)$
    - $t$ë¥¼ 1ì—ì„œ 0ìœ¼ë¡œ ì ì°¨ ì¤„ì—¬ë‚˜ê°€ë©´ì„œ ë‹¨ê³„ ë°˜ë³µ.

        mask predictor $p_{\theta}(x_0|x_t)$ë¥¼ ì‚¬ìš©í•´ maskingëœ token ì˜ˆì¸¡.
        ì˜ˆì¸¡ëœ í™•ë¥  ë¶„í¬ë¥¼ ì‚¬ìš©í•´ ê° tokenì´ í•´ë‹¹ ìœ„ì¹˜ì— ìˆì„ í™•ë¥  íŒŒì•….
        ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ tokenì„ ì„ íƒí•˜ê±°ë‚˜, í™•ë¥  ë¶„í¬ì— ë”°ë¼ ë¬´ì‘ìœ„ë¡œ token ì„ íƒ ê°€ëŠ¥.

    - Remasking : ì˜ˆì¸¡ëœ token ì¤‘ ì¼ë¶€ë¥¼ ë‹¤ì‹œ masking.
3. ìµœì¢… í…ìŠ¤íŠ¸ ìƒì„±

    $t=0$ì´ë©´ ìµœì¢… í…ìŠ¤íŠ¸ $x_0$ ìƒì„±.


### Accelerate:

- Pseudo-numerical solvers (DDIM)ì´ë‚˜ fewer-step sampling ê¸°ë²• ì‚¬ìš©.
diffusionì„ ê·¼ì‚¬í•˜ëŠ” ìˆ˜ì¹˜ í•´ë²•ì„ ì‚¬ìš©í•´ ë” ì ì€ ë‹¨ê³„ë¡œ ê³ í’ˆì§ˆ ìƒ˜í”Œ ìƒì„±.
diffusion ê³¼ì •ì˜ ê° ë‹¨ê³„ë¥¼ ìµœì í™”í•˜ê±°ë‚˜  ë¶ˆí•„ìš”í•œ ë‹¨ê³„ë¥¼ ì œê±°í•´ ì¶”ë¡  ì†ë„ í–¥ìƒ.
- ìµœê·¼ì—ëŠ” 5~10 stepë§Œìœ¼ë¡œë„ ì¶©ë¶„íˆ ë†’ì€ í’ˆì§ˆ ìƒì„±ì´ ê°€ëŠ¥í•œ Fast DLM ì—°êµ¬ë„ ì§„í–‰ ì¤‘.

### Scalability

<img src="image_6.png" alt="image_6.png" style="width:500px;height:auto;" />

## Reference

- [Large Language Diffusion Models (arXiv 2502.09992)](https://arxiv.org/abs/2502.09992)
- [Large Language Diffusion Models - Hugging Face Papers](https://huggingface.co/papers/2502.09992)
- [Is the Mercury LLM the first of a new generation of LLMs?](https://machine-learning-made-simple.medium.com/is-the-mercury-llm-the-first-of-a-new-generation-of-llms-b64de1d36029)
- [Paper Review: Large Language Diffusion Models - Andrey Lukyanenko](https://andlukyane.com/blog/paper-review-llada)
- [ML-GSAI/LLaDA: Official PyTorch implementation for Large Language Diffusion Models](https://github.com/ML-GSAI/LLaDA)
- [Inception Labs News](https://www.inceptionlabs.ai/news)
